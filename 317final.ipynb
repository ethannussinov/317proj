{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz #PyMuPDF, python package for efficient text extraction from PDFs\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the project is to be able to reliably input a pdf and extract the text from it. The function below, `read_pdf` is designed for just that, making use of the PyMuPDF package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    '''\n",
    "    This function takes an argument of the file path of a linguistic paper in PDF format and returns its text content.\n",
    "    '''\n",
    "    text_content = ''\n",
    "    \n",
    "    try:\n",
    "        # open the PDF file\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            # iterate over each page\n",
    "            for page in pdf:\n",
    "                # extract text from the page and add it to the content\n",
    "                text_content += page.get_text()\n",
    "    # error handling            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    return text_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of this function in action, taking in Professor Danis' paper as a PDF, extracting the text and printing a segment to confirm its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acent vowel deletes is slightly but significantly longer than a short vowel in non-deletional \n",
      "contexts (p < 0.001). In the configuration studied here, deletion occurs in the vowel of a CV \n",
      "verb when occurring before a V-initial direct object (/CV1 +V2 / → [CV2]). However, instead \n",
      "of full vowel deletion as it is previously analysed (e.g. Akinlabi and Oyebade 1987, Ola Orie \n",
      "and Pulleyblank 2002), a compensatory lengthening analysis is proposed based on this new \n",
      "phonetic evidence. The experimen\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/papers/paper1.pdf'\n",
    "paper_text = read_pdf(file_path)\n",
    "print(paper_text[500:1000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to preprocess the text -- remove all unnecessary aspects of the paper. I designed the function `preprocess_text` to specifically target references and acknowledgements to be dropped, as well as some basic normalization of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_content):\n",
    "    '''\n",
    "    This function takes a text argument and returns the text preprocessed, without references or acknowledgements, and with normalized text.\n",
    "    '''\n",
    "    # remove references section\n",
    "    text_content = re.sub(r'\\n?References\\n?.*', '', text_content, flags=re.DOTALL)\n",
    "    # remove any lines that contain URLs, DOI, or email addresses\n",
    "    text_content = re.sub(r'\\S*@\\S*\\s?', '', text_content)\n",
    "    text_content = re.sub(r'http\\S+', '', text_content)\n",
    "    text_content = re.sub(r'doi:\\S+', '', text_content)\n",
    "    # normalize text by converting to lowercase and removing special characters\n",
    "    # keep hyphenated terms and apostrophes within words\n",
    "    text_content = re.sub(r'[^a-zA-Z0-9\\s\\-\\']', ' ', text_content)\n",
    "    text_content = text_content.lower()\n",
    "    \n",
    "    # remove any double or more spaces with a single space\n",
    "    text_content = re.sub(' +', ' ', text_content)\n",
    "    \n",
    "    # strip whitespace at the beginning and end of the text\n",
    "    text_content = text_content.strip()\n",
    "\n",
    "    return text_content\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of this function in action, taking in the textualized PDF from the last step and cleaning it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " significantly longer than a short vowel in non-deletional \n",
      "contexts p 0 001 in the configuration studied here deletion occurs in the vowel of a cv \n",
      "verb when occurring before a v-initial direct object cv1 v2 cv2 however instead \n",
      "of full vowel deletion as it is previously analysed e g akinlabi and oyebade 1987 ola orie \n",
      "and pulleyblank 2002 a compensatory lengthening analysis is proposed based on this new \n",
      "phonetic evidence the experiment for this study controlled for inherent vowel duration \n",
      "vo\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = preprocess_text(paper_text)\n",
    "print(preprocessed_text[500:1000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to process the text. The function `process_text` makes use of several important NLTK models to tokenize and POS tag the preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_text(text):\n",
    "    '''\n",
    "    This function takes a text argument and returns the text tokenized and POS tagged, utilizing the above NLTK models\n",
    "\n",
    "    '''\n",
    "    # tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # POS tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    return tagged_tokens\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of this function in action, taking in the preprocessed text from the last step and tokenizing, and POS tagging it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('future', 'JJ'), ('research', 'NN'), ('.', '.'), ('2', 'CD'), ('.', '.'), ('Vowel', 'NNP'), ('deletion', 'NN'), ('process', 'NN'), ('In', 'IN'), ('discussing', 'VBG'), ('the', 'DT'), ('deletion', 'NN'), ('process', 'NN'), (',', ','), ('the', 'DT'), ('vowel', 'NN'), ('that', 'WDT'), ('remains', 'VBZ'), ('after', 'IN'), ('an', 'DT'), ('adjacent', 'JJ'), ('vowel', 'NN'), ('deletes', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('remnant', 'JJ'), ('vowel', 'NN'), ('(', '('), ('as', 'IN'), ('stated', 'VBN'), ('above', 'IN'), (')', ')'), ('.', '.'), ('Likewise', 'NNP'), (',', ','), ('a', 'DT'), ('short', 'JJ'), ('vowel', 'NN'), ('outside', 'IN'), ('of', 'IN'), ('deletion', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('“', 'JJ'), ('simple', 'JJ'), ('vowel', 'NN'), ('”', 'NNP'), ('.', '.'), ('Any', 'CC'), ('analysis', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_paper_text = process_text(paper_text)\n",
    "print(tagged_paper_text[500:550])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is where things get more difficult -- term extraction. I had several ideas on how I could go about this, and ended up going with a combination of a few of them.\n",
    "\n",
    "I knew one of the intial steps would have to be stopword filtering.\n",
    "\n",
    "My first idea was to identify noun phrases as potential terms (often consisting of a noun (NN, NNS), sometimes preceded by a determiner (DT), adjectives (JJ), or another noun in the event that it is a compound noun)\n",
    "\n",
    "Another idea was frequency filtering: looking for words that appear multiple times in the text, suggesting increased importance in the paper.\n",
    "\n",
    "Further research led me to Named Entity Recognition, which I thought could be a valuable addition to some of the other methods I had already thought up.\n",
    "\n",
    "A less solid idea was to check the text against a predefined list of linguistic terms, which would be less reliable but could help increase the validity of my approach by being one of the later steps in the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ethannussinov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def enhanced_term_identification(tagged_tokens, linguistic_terms_set=None):\n",
    "    '''\n",
    "    This function integrates various strategies to identify and filter terms for a glossary.\n",
    "    It incorporates Noun Phrase Extraction, Frequency Filtering, Stopword Removal,\n",
    "    Linguistic Term Filter, Named Entity Recognition (NER), and Heuristic Rules.\n",
    "\n",
    "    Parameters:\n",
    "    - tagged_tokens: a list of POS-tagged tokens from the text.\n",
    "    - linguistic_terms_set: an optional set of known linguistic terms for additional filtering.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary of filtered terms and their frequencies.\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    noun_phrases = []\n",
    "\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN|NNS>+}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    tree = cp.parse(tagged_tokens)\n",
    "\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        phrase = \" \".join(word for word, tag in subtree.leaves() if word.lower() not in stop_words and len(word) > 1)\n",
    "        if phrase:\n",
    "            noun_phrases.append(phrase)\n",
    "\n",
    "    term_counts = Counter(noun_phrases)\n",
    "\n",
    "    if linguistic_terms_set:\n",
    "        term_counts = {term: count for term, count in term_counts.items() if term in linguistic_terms_set}\n",
    "\n",
    "    filtered_terms = {term: count for term, count in term_counts.items() if count > 1 and len(term) > 3}\n",
    "\n",
    "    return filtered_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = enhanced_term_identification(tagged_paper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': 4,\n",
       " 'vowel deletion': 14,\n",
       " 'compensatory lengthening': 18,\n",
       " 'Evidence': 6,\n",
       " 'phonetics': 12,\n",
       " 'vowel': 15,\n",
       " 'adjacent vowel deletes': 2,\n",
       " 'short vowel': 13,\n",
       " 'configuration': 2,\n",
       " 'verb': 7,\n",
       " 'full vowel deletion': 2,\n",
       " 'compensatory': 4,\n",
       " 'analysis': 7,\n",
       " 'experiment': 3,\n",
       " 'study': 3,\n",
       " 'voicing': 3,\n",
       " 'manner': 2,\n",
       " 'articulation': 3,\n",
       " 'results': 12,\n",
       " 'tone': 8,\n",
       " 'direct object': 3,\n",
       " 'phonology': 6,\n",
       " 'deletion': 14,\n",
       " 'vowel duration': 3,\n",
       " 'vowel deletion process': 2,\n",
       " 'pilot study': 2,\n",
       " 'duration': 19,\n",
       " 'underived short vowel': 2,\n",
       " 'remnant vowel': 19,\n",
       " 'sequence': 7,\n",
       " 'tata': 2,\n",
       " 'grasshopper': 4,\n",
       " 'process': 10,\n",
       " 'full deletion': 5,\n",
       " 'difference': 6,\n",
       " 'account': 2,\n",
       " 'mora': 6,\n",
       " 'phonetic module': 2,\n",
       " 'http': 6,\n",
       " '//spilplus.journals.ac.za': 2,\n",
       " 'speaker': 4,\n",
       " 'vowels': 9,\n",
       " 'data': 2,\n",
       " 'contexts': 2,\n",
       " 'deletion process': 3,\n",
       " 'simple vowel': 3,\n",
       " 'simple short vowel': 4,\n",
       " 'standard phonological account': 5,\n",
       " 'word': 2,\n",
       " 'structure': 4,\n",
       " 'projects': 3,\n",
       " 'phonological account': 4,\n",
       " 'standard account': 7,\n",
       " 'form': 4,\n",
       " 'phonetic duration': 2,\n",
       " 'case': 7,\n",
       " 'native speaker': 2,\n",
       " 'language': 6,\n",
       " 'subject': 7,\n",
       " 'quality': 3,\n",
       " 'phonetics http': 5,\n",
       " 'time': 2,\n",
       " 'words': 5,\n",
       " 'elicitation': 2,\n",
       " 'groups': 4,\n",
       " 'sentences': 3,\n",
       " 'page': 3,\n",
       " 'pause': 2,\n",
       " 'durations': 6,\n",
       " 'cripple': 2,\n",
       " 'bean cake': 2,\n",
       " 'scorpion': 2,\n",
       " 'hand': 2,\n",
       " 'stool': 2,\n",
       " 'pepper': 3,\n",
       " 'make-up powder': 2,\n",
       " 'wood': 2,\n",
       " 'field': 2,\n",
       " 'file': 2,\n",
       " 'vowel offset': 2,\n",
       " 'cases': 3,\n",
       " 'verbs': 4,\n",
       " 'significant difference': 3,\n",
       " 'remnant vowels': 8,\n",
       " 'means': 3,\n",
       " 'Differences': 2,\n",
       " 'short vowels': 3,\n",
       " 'simple vowels': 4,\n",
       " 'number': 2,\n",
       " 'tokens': 2,\n",
       " 'word length': 2,\n",
       " 'forms': 4,\n",
       " 'type': 2,\n",
       " 'segments': 2,\n",
       " 'phonetic implementation': 3,\n",
       " 'incomplete neutralisation': 7,\n",
       " 'phonetic differences': 2,\n",
       " 'revised phonological account': 3,\n",
       " 'root node deletion': 2,\n",
       " 'lengthening': 3,\n",
       " 'output': 2,\n",
       " 'mora reassociates': 2,\n",
       " 'phonetic realisation': 4,\n",
       " 'long vowel': 5,\n",
       " 'claim': 2,\n",
       " 'long vowels': 4,\n",
       " 'contrastive long vowels': 2,\n",
       " 'bimoraic vowels': 2,\n",
       " 'point': 2,\n",
       " 'Thanks': 3,\n",
       " 'discussion': 2,\n",
       " 'bimoraic structure': 2,\n",
       " 'vowel length contrasts': 2,\n",
       " 'low functional load': 2,\n",
       " 'phonological module outputs': 2,\n",
       " 'context': 2,\n",
       " 'standard approach': 2,\n",
       " 'revised approach': 2,\n",
       " 'clear reason': 2,\n",
       " 'conflict': 2,\n",
       " 'paradigm': 3,\n",
       " 'particle': 3,\n",
       " 'isolation': 3,\n",
       " 'blood': 2,\n",
       " 'phonological structure': 2,\n",
       " '=TARGETDUR': 2,\n",
       " 'cost': 3,\n",
       " 'contrastive categories': 2,\n",
       " 'fact': 2,\n",
       " 'syntactic environment': 2,\n",
       " 'processes': 3,\n",
       " 'assimilation': 2,\n",
       " 'sequences': 2,\n",
       " 'https': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ac3b6dd44aaff0aa9ed65164304391991eb7be0aaf461540e996ffd4cd9c15d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
