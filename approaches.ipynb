{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Linguistics Final Project\n",
    "### Automatic Glossary Creator\n",
    "\n",
    "The idea for my final project is a python workflow that reads in an academic paper, identifies relevant/important terms, then creates a glossary where it generates definitions for each.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_pdf, preprocess_text, process_text, extract_terms_with_tfidf, enhanced_term_identification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: TF-IDF\n",
    "\n",
    "The idea with my initial approach was to extract terms using tf-idf. This involved processing the text extracted from the academic papers to identify terms that were 'statistically significant' within the document.\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "### Pros: \n",
    "1. Automated, quantitative: works well at scale.\n",
    "2. Easy to establish.\n",
    "\n",
    "### Cons: \n",
    "1. No contextual understanding: elevates general or irrelevant terms that show up a lot, but aren't necessarily significant.\n",
    "2. Overemphasis on common words: even outside of stop words, this approach prioritizes obscure or infrequently used words that arent useful for term identification approaches.\n",
    "\n",
    "\n",
    "This approach likely could've been refined more to work better, and it is something that I will look into more over the next week, but at this point I decided to move on to a new approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'al', 'aj', 'additionally', 'approach', 'analysed', 'assimilation', 'articulation', 'assume', 'akinlabi', 'adjacent', 'appear', 'ak', 'akin', 'anonymous', 'anderson', 'ap', 'akg', 'alternate', 'amp', 'ambiguous', 'alternatively', 'acoustic', 'acknowledgements', 'adebola', 'accounts', 'acal', 'absolute', 'actually', 'aided', 'aid', 'advanced', 'amplitude', 'argued', 'analyses', 'analyse', 'assistance', 'assimilates', 'aaron']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/papers/paper1.pdf'\n",
    "paper_text = read_pdf(file_path)\n",
    "preprocessed_text = preprocess_text(paper_text)\n",
    "\n",
    "\n",
    "tfidf_terms = extract_terms_with_tfidf(preprocessed_text)\n",
    "\n",
    "print(tfidf_terms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: The Conglomerate Approach\n",
    "\n",
    "The idea with this second approach was to combine a variety of different techniques that I had thought would all be useful, and see if combining them would lead to more effective results.\n",
    "\n",
    "## Workflow:\n",
    "1. Input preparation through `read_pdf` function\n",
    "2. Text preprocessing through `preprocess_text`, removing references, URLs, email addresses, special characters, normalizing text (through ocnverting to lowercase), removing extraneous spaces and stripping whitespace.\n",
    "3. Term extraction setup through POS tagging, utilizing the `process_text` function, which in turn utilizes NLTK tokenizer and POS tagger. Extremely crucial for identification of noun phrases moving forward.\n",
    "4. Noun phrase extraction. A specific grammar pattern (`\"NP: {<DT>?<JJ>*<NN|NNS>+}\"`) is defined to identify noun phrases. This pattern optionally looks for determiners, any number of adjuectives, followed by one or more nouns. Using the defined grammar, a parser (in this case, nltk.RegexpParser) is used to parse the tagged tokens and extract sequences that match the noun phrase grammar.\n",
    "5. Term identification and filtering -- extracted noun phrases are collected. Each phrase undergoes further filtering to exclude stopwords and single characters, ideally focusing on phrases that meaningfully represent concepts. The frequency of each noun phease is cacluated, counted and stored, alloweing for frequency-based filtering.\n",
    "\n",
    "\n",
    "### Pros:\n",
    "1. Contextual relevance: by extracting noun phrases, this approach better captures the context in which the terms we seek to extract are used.\n",
    "2. Domain specific filtering: Incorporating this allows for terms more relevant to the specified field, in this case, linguistics.\n",
    "3. Heuristic rules: filtering by length and frequency, allows us to eliminate irrelevant or less important terms\n",
    "4. Noise reduction through stopword filtering and single character filtering.\n",
    "\n",
    "### Cons:\n",
    "1. Complexity: Sort of a mishmash, computationally more complex and generally harder to follow.\n",
    "2. Dependency on tagged tokens: The quality of our results depends heavily on the accuracy of our POS tagging (something that is generally going to be out of my control, reliant on preexisting libraries), but errors in tagging would heavily impact the results of my model.\n",
    "3. Overfiltering: Issue I haven't explored too much\n",
    "4. Maintenance of the linguistic term set: the need to maintain/update the term set adds administrative burden -- problem I hope to circumvent by web scraping significant term websites.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'work': 4, 'evidence': 6, 'phonetics': 14, 'yor vowel deletion': 4, 'vowel': 16, 'adjacent vowel deletes': 2, 'short vowel': 12, 'deletion': 13, 'full vowel deletion': 2, 'akinlabi': 3, 'oyebade': 2, 'ola orie': 7, 'pulleyblank': 10, 'compensatory': 10, 'analysis': 5, 'experiment': 2, 'study': 3, 'manner': 2, 'articulation': 3, 'results': 12, 'vowel duration': 2, 'vowel deletion process': 3, 'pilot study': 2, 'duration': 18, 'remnant vowel': 16, 'vv sequence': 3, 'tata': 2, 'process': 10, 'full deletion': 5, 'difference': 6, 'account': 3, 'mora': 5, 'remnant': 2, 'phonetic module': 2, 'speaker': 4, 'vowels': 6, 'data': 3, 'contexts': 2, 'research': 2, 'deletion process': 2, 'simple vowel': 2, 'simple short vowel': 4, 'standard phonological account': 6, 'structure': 4, 'projects': 3, 'standard account': 5, 'form': 4, 'phonetic duration': 2, 'case': 6, 'native speaker': 2, 'language': 6, 'subject': 7, 'verb': 5, 'quality': 3, 'analysis yor vowel deletion': 2, 'time': 2, 'elicitation': 2, 'groups': 4, 'sentences': 3, 'page': 3, 'broselow chen': 2, 'huffman': 2, 'pause': 2, 'full wordlist': 2, 'field': 2, 'phonology': 3, 'cases': 3, 'verbs': 2, 'durations': 5, 'remnant vowels': 10, 'simple vowels': 5, 'significant difference': 3, 'means': 5, 'figure': 2, 'differences': 3, 'short vowels': 5, 'ms stddev': 2, 'words': 4, 'number': 2, 'word length': 2, 'forms': 4, 'type': 2, 'tone': 4, 'implications': 2, 'phonetic implementation': 3, 'incomplete neutralisation': 8, 'phonetic differences': 2, 'revised phonological account': 5, 'root node deletion': 2, 'compensatory lengthening': 3, 'lengthening': 2, 'output': 2, 'vowel deletion': 7, 'mora reassociates': 2, 'phonetic realisation': 4, 'luganda': 3, 'long vowel': 5, 'chene': 2, 'anderson': 2, 'claim': 2, 'long vowels': 5, 'connection': 2, 'contrastive long vowels': 2, 'bimoraic vowels': 2, 'point': 2, 'thanks': 3, 'discussion': 2, 'bimoraic structure': 2, 'hindi': 2, 'vowel length contrasts': 2, 'low functional load': 2, 'phonological module outputs': 2, 'braver': 2, 'context': 2, 'standard approach': 2, 'revised approach': 2, 'clear reason': 2, 'conflict': 2, 'paradigm': 2, 'particle': 3, 'isolation': 3, 'social status': 2, 'phonological structure': 2, 'cost': 3, 'contrastive categories': 2, 'fact': 2, 'syntactic environment': 2, 'aj et al': 4, 'processes': 3, 'hl-simplification': 2, 'assimilation': 3, 'sequence': 2}\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/papers/paper1.pdf'\n",
    "paper_text = read_pdf(file_path)\n",
    "preprocessed_text = preprocess_text(paper_text)\n",
    "tagged_tokens = process_text(preprocessed_text)\n",
    "\n",
    "\n",
    "terms = enhanced_term_identification(tagged_tokens)\n",
    "\n",
    "print(terms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "1. Refinement of the Congolmerate Approach -- incorporation of the term set, improvement on heuristic rules, more focus on Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed\n",
    "\n",
    "\n",
    "def generate_definition_gpt2(term):\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    prompt = f\"Define the term '{term} in the context of an academic linguistic paper\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', max_length=150)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=200,\n",
    "        max_new_tokens=70, \n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    definition = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Definition Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=70) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Definition: Define the term 'vowel deletion in the context of an academic linguistic paper'.\n",
      "\n",
      "In this case, it is important to note that this is not the first time that academic papers have been deleted in this way. In fact, the most recent example of this was published in The Journal of the American Academy of Child and Adolescent Psychiatry in 2012, where a paper was deleted by the journal's editors after it had been\n"
     ]
    }
   ],
   "source": [
    "term = \"vowel deletion\"\n",
    "definition = generate_definition_gpt2(term)\n",
    "print(\"Generated Definition:\", definition)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress\n",
    "\n",
    "Much more nitpicky than initially expected. I thought I'd be able to just throw in a ChatGPT prompt, but I will have to do some proper prompt engineering, as well as fine tuning temperature (as I understand it, the seriousness of the model), and the other changeable settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions/Advice?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usage of .lower()\n",
    ".casefold()\n",
    "- more aggressive version of lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ac3b6dd44aaff0aa9ed65164304391991eb7be0aaf461540e996ffd4cd9c15d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
